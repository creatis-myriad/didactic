_target_: vital.models.attention.transformer.Transformer

d_token: 192
n_bidirectional_blocks: 0
n_self_blocks: 3
attention_n_heads: 8
attention_dropout: 0.2
attention_initialization: kaiming
attention_normalization:
  _target_: torch.nn.LayerNorm
  normalized_shape: ${task.model.encoder.d_token}
  _partial_: True
ffn_d_hidden: ${task.model.encoder.d_token}
ffn_dropout: 0.1
ffn_activation:
  _target_: vital.models.attention.layers.ReGLU
  _partial_: True
ffn_normalization:
  _target_: torch.nn.LayerNorm
  normalized_shape: ${task.model.encoder.d_token}
  _partial_: True
residual_dropout: 0.1
prenormalization: True
first_prenormalization: False
